## 网络编程

### I/O 多路复用

https://mp.weixin.qq.com/s/Qpa0qXxuIM8jrBqDaXmVNA

##### socket

##### select、poll

##### epoll



### Reactor和Proactor

需要记住的是：

##### Reactor模式

1. **同步非阻塞I/O**：Reactor模式通常用于同步非阻塞I/O操作。在这种模式下，I/O操作（如读或写）在被调用时不会立即完成，而是立即返回，稍后当I/O操作可以继续进行时，事件处理器会被调用。
2. **事件通知**：当I/O操作准备就绪时，Reactor会通知相应的事件处理器（或回调函数）来处理事件。
3. **用户空间处理**：在Reactor模式中，一旦事件被触发，事件处理器需要在用户空间完成所有的I/O操作，包括从非阻塞I/O调用中获取数据。
4. **事件循环**：Reactor模式通常有一个事件循环，不断地检查I/O事件，并分派事件处理器。

##### Proactor模式

1. **异步I/O**：Proactor模式用于异步I/O操作，这意味着I/O操作（如读或写）被发起后，操作系统会接管并完成I/O操作，而不需要用户程序的进一步干预。
2. **操作系统完成I/O**：在Proactor模式中，**I/O操作是由操作系统在后台线程中完成**的，当操作完成后，操作系统会通知应用程序。
3. **回调在操作系统层面**：Proactor模式中的回调通常在操作系统层面被触发，这意味着当I/O操作完成时，操作系统会直接调用回调函数。
4. **用户空间的简单性**：由于I/O操作是由操作系统完成的，用户程序在处理I/O完成事件时通常只需要处理已经完成的数据，而不需要关心I/O操作的具体细节。
5. **事件处理的延迟**：在Proactor模式中，由于I/O操作是由操作系统完成的，因此事件处理可能会有一些延迟，因为操作系统需要时间来完成I/O操作。



**总结：**

- **Reactor模式**：适用于同步非阻塞I/O，用户程序需要在用户空间完成I/O操作，事件处理器在I/O操作准备就绪时被调用。
- **Proactor模式**：适用于异步I/O，操作系统完成I/O操作，事件处理器在I/O操作完成后被调用。

**Proactor模式通常能够提供更好的性能，特别是在高并发场景下，因为它减少了用户程序需要执行的I/O操作数量**，并将I/O操作的完成处理委托给了操作系统。然而，Proactor模式的实现通常比Reactor模式更复杂，并且需要操作系统的支持。



:thought_balloon:**联想：**boost库的asio ， 在处理async读写等操作，其handler是在完成IO之后的操作，而不是去执行IO操作的函数



### 协议

#### HTTP

##### 长轮询、短轮询、websocket

https://blog.csdn.net/pacosonswjtu/article/details/52035252#:~:text=1%EF%BC%89%E4%BB%8B%E7%BB%8D%EF%BC%9A%20http

**短轮询** 是指客户端在固定的时间间隔内不断向服务器发送请求，**无论服务器是否有新的数据更新**，服务器都会**立即返回当前的状态**（**很可能数据并没有准备好）**。这种方式简单易实现，但可能会导致大量的无效请求，从而增加服务器的负担和网络延迟。

**长轮询** 是一种改进的轮询方式，客户端向服务器发送请求后，如果服务器没有新的数据，它会保持连接打开状态，**直到有数据可用或者超时**。这种方式可以减少不必要的网络请求，提高实时性，但服务器需要保持连接状态，可能会占用更多的服务器资源。

【补充】http 长连接（tcp 连接可复用）

> http协议 目前有两个版本：1.1 和 1.0；区别是 1.1支持 长连接（普遍使用http1.1版本），长连接也叫做持久连接（keep-alive）；而1.0不支持长连接，在1.0版本下，每个http请求响应后都会关闭tcp连接，下一次http请求会重新建立http连接；

2）**http 长连接**：多个http 请求共用同一个 tcp 连接，这样可以减少相邻多次 http请求导致的 tcp连接建立和关闭的资源消耗；http1.1 在请求头和响应头中用 connection 字段标识 该http连接是否是 长连接，即**connection: keep-alive 表示长连接**；而 connection: closed 表明服务器关闭tcp 连接；
3）**keep-alive**：与 connection 相对应的是 keep-alive，其属性有 **timeout=30 和 max=5** 分别是 两次 http 请求 保持的时间，max表示这个tcp 连接最多被几个 http 请求重用；

---

#### **WebSocket**

定义了一种在通过一个单一的 socket 在网络上进行**全双工通讯的通道**。

>  WebSocket 是 html5 规范发布的新协议，和 http协议完全是两个不同的概念，或者说基本没关系；WebSocket 协议 和 http协议的唯一联系点在于，WebSocket 协议为了兼容现有浏览器的握手规范而采用了 http协议中的握手规范 以建立WebSocket连接；

WebSocket 协议允许客户端和服务器之间建立一个持久的连接，**服务器**可以通过这个连接**主动向客户端推送数据**（类似于推送）。这种方式提供了真正的实时通信，减少了延迟和网络流量，适合需要高度实时性的应用程序。

解决的问题：

1. **http协议的被动性**：采用 WebSocket 协议后，服务器可以主动推送消息给客户端；而不需要客户端以（长/短）轮询的方式发起http请求到server以获取数据更新反馈；这样一来，**客户端只需要经过一次HTTP请求**，就可以做到源源不断的信息传送了（在程序设计中，这种设计叫做回调，即：server 端有信息了再来通知client 端，而不是 client 端 每次都傻乎乎地跑去轮询server端 是否有消息更新）；
2. **http协议的无状态性/健忘性**：短轮询是每次http请求前都要建立连接，而长轮询是相邻几次请求前都要建立连接；http请求响应完成后，服务器就会断开连接，且把连接的信息全都忘记了；所以每次建立连接都要重新传输连接上下文（下面有补充），将 client 端的连接上下文来告诉server 端；而 WebSockct只需要一次HTTP 握手，整个通讯过程是建立在一次连接（状态）中的，server 端会一直推送消息更新反馈到客户端，直到客户端关闭请求，这样就无需 客户端为发送消息而建立不必要的 tcp 连接 和 为了建立tcp连接而发送不必要的冗余的连接上下文消息；

---

### 设计思路

> 参考boost库学习 https://gitbookcpp.llfc.club/sections/cpp/boost/asio05.html

#### :warning: 同步服务器(生产不用)

- **handleFunc** :用于处理请求服务
- **acceptor** :用于接受请求

为什么不用？

1. 同步读写的缺陷在于读写是**阻塞的**，如果客户端对端不发送数据服务器的read操作是阻塞的，这将导致服务器处于阻塞等待状态。
2.  可以通过**开辟新的线程**为新生成的连接处理读写，但是一个进程开辟的线程是**有限的**，约为2048个线程，在Linux环境可以通过unlimit增加一个进程开辟的线程数，但是线程过多也会导致切换消耗的时间片较多。 
3. 该服务器和客户端为**应答式**，实际场景为全双工通信模式，发送和接收要独立分开。 
4. 该服务器和客户端未考虑粘包处理。 综上所述，是我们这个服务器和客户端存在的问题

但有一个思路可以借鉴

用队列保证

---

> 为解决上述问题，我们在接下里的文章里做不断完善和改进，主要以异步读写改进上述方案。 当然同步读写的方式也有其优点，比如客户端连接数不多，而且服务器并发性不高的场景，可以使用同步读写的方式。使用同步读写能简化编码难度。

#### ⭐异步服务器

> 参考boost学习https://gitbookcpp.llfc.club/sections/cpp/boost/asio06.html

**必要组件**

**Server：**

- 监听，async_accept
- 接受连接 , acceptHandler
- 创建、回收会话 start_accept 、doClose

**Session:**

- 读 doRead readHandler
- 写 doWrite writeHandler
- 业务 service 也就是服务器为什么称之为服务器的地方



##### 问题处理

1. 如何全双工通讯

   读队列+写队列来实现全双工通讯。不用单队列+符号的原因是

2. 如何健壮地处理空间

   - 读、写、监听，用户端关闭，产生的多次析构问题

     思路：

     > https://gitbookcpp.llfc.club/sections/cpp/boost/asio08.html
     
     - 在读写监听都对异常有处理的情况：延长Session的生命周期defer，保证Session在退出handler前的存活，最后引用计数为0时，由智能指针释放空间
     
     如果我交由监听全权负责释放空间，读写只抛出异常，不处理会有什么问题？
     
     例：当我有两个函数都抛出了异常（这在多线程、异步的程序里都很常见），就会产生double free的错误。

3. 粘包问题

   > 何为粘包问题？
   >
   > 由于发送缓冲区和接受缓冲区的机制和大小的区别，会导致在读取时可能发送多个包粘连在一起读取的情况

   ![https://cdn.llfc.club/1683373951566.jpg](.\picture\1683373951566.jpg)
   
   :warning: 在处理接收数据的时候，`_data`与接收缓冲区直接相关，更偏下层，建议与处理`buffer`进行区分开来，不要直接对`_data`进行直接的解析操作，会增加代码复杂度，且只对纯应用层的对象进行操作，更符合分层思想

​	

#### 解耦

> 对于一个庞大的系统来说，耦合是一个很需要注意的问题，如何解耦也是一个需要的能力

##### 分层思想

分层思想贯穿计算机设计中，将任务区分层数，来规范职责，这也是解耦的一大基础

例：我们常常在网络层完成asio 的 io操作，而将收到数据后的业务处理放在业务层、逻辑层。将数据丢进消息队列，逻辑层消费数据再决定是否需要io操作，这样可以避免伪阻塞的问题----handler调用下一步注册io的问题。当然这里的消费过程依旧可以区分层数，比如根据iso模型的话可以分为应用层和会话层，在处理更加复杂的问题时，分层是可以将处理过程更加清晰，后续拓展性也会更好

对应boost学习：https://gitbookcpp.llfc.club/sections/cpp/boost/asio17.html

![https://cdn.llfc.club/1685621283099.jpg](./picture/1685621283099.jpg)



## 并发编程

> 《C++ 并发编程》

### 线程

##### thread

> 启动

```c++
std::thread t(funToken);
```

:warning:引用类型参数的传递：创建一个`std::thread`对象并传递参数给线程函数时，如果参数是引用类型，**必须显式地使用`std::ref`或`std::cref`来传递引用**。这是因为`std::thread`的构造函数会将参数进行值传递，而不是引用传递。如果不显式地使用`std::ref`或`std::cref`，传递的将是一个副本，而不是原始对象的引用。

##### join

> 启用该函数的主线程，阻塞等待在该线程创建的子线程完成

```c++
t.join();//等待t线程的结束
```

##### detach

> 线程允许采用分离的方式在后台独自运行，`C++ concurrent programing`书中称其为守护线程。

```c++
t.detach();
```

**tips**：

- 主线程崩溃时，需要考虑到子线程的影响，例如充值线程，一定执行后再回收资源
- :warning:慎用局部变量的引用、指针，在并发编程中是一个非常危险的操作
- :warning:慎用隐式转换
- thread **无法拷贝构造 、拷贝赋值**  ，可以使用移动语义 



---

### 互斥

#### 互斥量

- mutex 不允许拷贝、移动构造；移动、拷贝赋值；唯一；
- recursive_mutex 递归互斥量，不推荐使用
- shared_mutex 共享互斥量 支持共享（读）和独占（写）两种模式的互斥量。

#### 智能锁

- unique_lock
- lock_guard
- 

####  死锁

##### 产生

就是多个线程在持有锁的同时请求其他锁 

##### 解决方案

1. 减少多种锁在函数中的交叉加锁，例如只有一个类型会获取多个锁，且单个锁互相独立，没有先决条件
2. **同时加锁**：

   1. 利用std::lock(m1,m2);可以同时对两个互斥量进行上锁 
   2. 利用std::scope_lock(m1,m2)    (>=std c++17)

   存在一定问题，因为在实际开发过程中，多个锁同时请求的情况并不被允许，所以还是无法避免同一个函数需要非同时加多个锁的情况
3. 层级锁、权重锁，简单来说就是给锁增加优先级的属性，**彻底预防死锁**，但灵活性弱，实现复杂，适合在调试时使用。

   核心思想是：**为所有锁分配一个层级编号，强制线程按照从高到低的顺序获取锁**，若违反顺序则直接抛出异常或终止程序，从而避免循环等待导致的死锁。

   层级锁设计，并不是一层一层递进上锁，虽然这样也可以实现预防死锁，但这也使得程序趋近于线形而非并发。





#### 单例模式

单例模式模式的演变过程

===c++11 -==

##### 饿汉模式

> 在调用之前，在主线程将类其初始化。解决重复初始化问题

优点：是线程安全的

缺点：约束开发人员行为，在未使用的情况下依旧有内存损耗

饿汉式是**从使用角度规避多线程**的安全问题，很多情况下我们很难从规则角度限制开发人员



**懒汉模式**

> 设计思路，想要在使用的时候再初始化，减少开发人员约束

操作：给初始化操作上锁，判断是否已经初始化

优点：减少内存损耗

缺点：双重检查锁模式存在线程安全问题，且无法合理的回收资源



##### **懒汉模式+智能指针**

> 通过编写辅助删除器类 来构造智能指针，来解决回收资源的问题

优点：解决了double free的风险

缺点：依旧是线程不安全的



===c++11+==

##### 局部静态变量模式

> c++ 11以后，利用“当一个函数中定义一个局部静态变量，那么这个局部静态变量只会初始化一次”的这个特性，设计单例模式，线程安全

优点：简单、线程安全

缺点：实例存放在静态区，无法手动释放实例，只能等待程序结束。在类较大的情况下，占据一定资源。



##### call_once

> C++11 提出了`call_once`函数，我们可以配合一个局部的静态变量`once_flag`实现线程安全的初始化。多线程调用`call_once`函数时，会判断`once_flag`是否被初始化，如没被初始化则进入初始化流程，调用我们提供的初始化函数。但是同一时刻只有一个线程能进入这个初始化函数。设计模式继承于**懒汉模式**

优点：优秀、线程安全、可做模板



##### Questions

- **Q：**为什么单例模式必须使用指针成员变量

- **A：**因为不能类自己包含自己啊

---

### 同步

> **同步（Synchronization）** 是计算机科学中协调多个执行单元（如线程、进程、设备）访问共享资源或协作完成任务的核心机制，其核心目标是**确保数据一致性、避免竞态条件（Race Condition）**，并维护程序的正确性。简单来说就是，在保证一定顺序规则的情况下，完成数据的操作。

#### 条件变量

> 程序员可以通过编程来实现同步，但由于“层数”太高，难免会导致资源的浪费，**条件变量**就是为了更优秀的实现同步而存在。

**std::condition_variable**

TODO：实现一个线程安全的队列



---

### 并发how？

##### 异步async

> `std::async` 是一个用于异步执行函数的模板函数，它返回一个 `std::future` 对象，该对象用于获取函数的返回值。

```c++
std::string fetchDataFromDB(std::string query) {
    // 模拟一个异步任务，比如从数据库中获取数据
    std::this_thread::sleep_for(std::chrono::seconds(5));
    return "Data: " + query;
}
//调用
std::future<std::string> resultFromDB = std::async(std::launch::async, fetchDataFromDB, "Data");
// 在主线程中做其他事情
std::cout << "Doing something else..." << std::endl;
// 从 future 对象中获取数据
std::string dbData = resultFromDB.get();
std::cout << dbData << std::endl;
```

**async的执行机制：**

- **`std::launch::async`**：强制在新线程中执行任务。
- **`std::launch::deferred`**：延迟执行，直到调用 `future.get()` 时在调用线程中同步执行。

可以发现异步的实现基于多线程，而一次async调用函数，也就意味着一次创销线程。（当然std内部自然有线程池来减少这种损耗）。

---

##### packaged_task

> - **任务绑定**：将任意可调用对象（函数、Lambda、成员函数等）封装为一个任务，并自动绑定一个 `std::future`，用于获取任务的最终结果或异常。
> - **延迟执行**：任务不会自动启动，需手动传递给线程、线程池或其他执行上下文。

`packaged_task`常和线程池配合使用。仅支持移动拷贝。

```c++
int my_task() {
    std::this_thread::sleep_for(std::chrono::seconds(5));
    std::cout << "my task run 5 s" << std::endl;
    return 42;
}
void use_package() {
    // 创建一个包装了任务的 std::packaged_task 对象  
    std::packaged_task<int()> task(my_task);
    // 获取与任务关联的 std::future 对象  
    std::future<int> result = task.get_future();
    // 在另一个线程上执行任务  
    std::thread t(std::move(task));
    t.detach(); // 将线程与主线程分离，以便主线程可以等待任务完成  
    // 等待任务完成并获取结果  
    int value = result.get();
    std::cout << "The result is: " << value << std::endl;
}
```

其用法其实和async没有什么区别，只是在线程分配和处理上更加自由。

---

##### future

> 一个模板类-容器，用于接收异步调用的return值 

1. **std::future::get()**:

   `std::future::get()` 是一个阻塞调用，用于获取 `std::future` 对象表示的值或异常。如果异步任务还没有完成，`get()` 会阻塞当前线程，直到任务完成。如果任务已经完成，`get()` 会立即返回任务的结果。重要的是，`get()` 只能调用一次，因为它会**移动或消耗**掉 `std::future` 对象的状态。一旦 `get()` 被调用，`std::future` 对象就不能再被用来获取结果。

2. **std::future::wait()**:

   `std::future::wait()` 也是一个阻塞调用，但它与 `get()` 的主要区别在于 `wait()` 不会返回任务的结果。它只是等待异步任务完成。如果任务已经完成，`wait()` 会立即返回。如果任务还没有完成，`wait()` 会阻塞当前线程，直到任务完成。与 `get()` 不同，`wait()` **可以被多次调用**，它不会消耗掉 `std::future` 对象的状态。

---

##### shared_future

> 适用于多个线程等待一个线程异步操作的结果的场景

 实现了拷贝赋值，而不是向future只支持移动赋值，所以可以重复使用用 实现共享

```c++
std::promise<int> promise;
std::shared_future<int> future = promise.get_future();
std::thread myThread1(myFunction, std::move(promise)); // 将 promise 移动到线程中
// 使用 share() 方法获取新的 shared_future 对象  
std::thread myThread2(threadFunction, future);
std::thread myThread3(threadFunction, future);
```

`shared_future`支持拷贝 ，实现类似于`shared_ptr`增加引用计数。

---

##### promise

> 由于主线程需要根据子线程的运行状态，来进行操作，但又不能等子线程完全结束后在get 所以有了promise `promise`一个模板类 手动设置异步操作的结果（值或异常），并通过关联的 `std::future` 传递给消费者

-  `set_value()` 设置一个值，由future容器去承接
- `set_exception()` 设置异常
- `get_future()`：获取关联的 `std::future` 对象。

---

##### 线程池

- 什么情况适合使用线程池
  - 任务无序
  - 任务无强关联性，共享空间少，上锁少
- 

##### **Tips**

- 注意区分同步（**Synchronous**）、异步 ；互斥、同步（**Synchronization**）的区别
  - **异步/同步的“同步”**是纵向的（单个任务的执行流程）。
  - **互斥/同步的“同步”**是横向的（多个任务间的交互）。
- 默认情况下，`std::async`使用`std::launch::async | std::launch::deferred`策略。这意味着任务可能异步执行，也可能延迟执行，具体取决于实现。需要注意的是，不同的编译器和操作系统可能会有不同的默认行为。
- 
